version = "v2ProPlus"
model_version = "v2ProPlus" 

import voice_management
# import psutil
# import os

# def set_high_priority():
#     """æŠŠå½“å‰ Python è¿›ç¨‹è®¾ä¸º HIGH_PRIORITY_CLASS"""
#     if os.name != "nt":
#         return # ä»… Windows æœ‰æ•ˆ
#     p = psutil.Process(os.getpid())
#     try:
#         p.nice(psutil.HIGH_PRIORITY_CLASS)
#         print("å·²å°†è¿›ç¨‹ä¼˜å…ˆçº§è®¾ä¸º High")
#     except psutil.AccessDenied:
#         print("æƒé™ä¸è¶³ï¼Œæ— æ³•ä¿®æ”¹ä¼˜å…ˆçº§ï¼ˆè¯·ç”¨ç®¡ç†å‘˜è¿è¡Œï¼‰")
# set_high_priority()
import json
import logging
import os
import re
import sys
import traceback
import warnings

import torch
import torchaudio
from text.LangSegmenter import LangSegmenter

logging.getLogger("markdown_it").setLevel(logging.ERROR)
logging.getLogger("urllib3").setLevel(logging.ERROR)
logging.getLogger("httpcore").setLevel(logging.ERROR)
logging.getLogger("httpx").setLevel(logging.ERROR)
logging.getLogger("asyncio").setLevel(logging.ERROR)
logging.getLogger("charset_normalizer").setLevel(logging.ERROR)
logging.getLogger("torchaudio._extension").setLevel(logging.ERROR)
logging.getLogger("multipart.multipart").setLevel(logging.ERROR)
warnings.simplefilter(action="ignore", category=FutureWarning)

# í•˜ë“œì½”ë”© ê²½ë¡œ (voice_managementì—ì„œ ìºë¦­í„°ë³„ë¡œ ê´€ë¦¬)
cnhubert_base_path = './pretrained_models/chinese-hubert-base'
bert_path = './pretrained_models/chinese-roberta-wwm-ext-large'

# Zero-shotìš© pretrained ëª¨ë¸ ê²½ë¡œ
PRETRAINED_SOVITS_PATH = "./pretrained_models/v2Pro/s2Gv2ProPlus.pth"
PRETRAINED_GPT_PATH = "./pretrained_models/s1v3.ckpt"

# ê¸°ë³¸ ì„¤ì •
is_half = True
device = 'cuda'
dtype = torch.float16 if is_half else torch.float32

# êµ¬ë¶„ì ì§‘í•©
splits = {"ï¼Œ", "ã€‚", "ï¼Ÿ", "ï¼", ",", ".", "?", "!", "~", ":", "ï¼š", "â€”", "â€¦"}

# GPT ê´€ë ¨ ì „ì—­ë³€ìˆ˜
hz = 50
max_sec = 30  # ê¸°ë³¸ê°’, change_gpt_weightsì—ì„œ ì—…ë°ì´íŠ¸ë¨
cache = {}
tts_idx = 0  # íŒŒì¼ ì¶©ëŒ ë°©ì§€ìš© ì¸ë±ìŠ¤

# LRU Cache Configuration
MAX_CACHED_ACTORS = 10

class LRUModelCache:
    """LRU(Least Recently Used) ê¸°ë°˜ ëª¨ë¸ ìºì‹œ ê´€ë¦¬ í´ë˜ìŠ¤"""
    def __init__(self, max_size=10, cache_type="unknown"):
        from collections import OrderedDict
        self.cache = OrderedDict()
        self.max_size = max_size
        self.cache_type = cache_type
    
    def get(self, key):
        if key not in self.cache:
            return None
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def put(self, key, model):
        if key in self.cache:
            self.cache.move_to_end(key)
            self.cache[key] = model
            return
        if len(self.cache) >= self.max_size:
            # pretrained ëª¨ë¸ì€ eviction ëŒ€ìƒì—ì„œ ì œì™¸
            for oldest_key in list(self.cache.keys()):
                if oldest_key != 'pretrained':
                    oldest_model = self.cache.pop(oldest_key)
                    print(f"[LRU] Evicting {self.cache_type} model: {oldest_key}")
                    del oldest_model
                    torch.cuda.empty_cache()
                    break
        self.cache[key] = model
    
    def keys(self):
        return list(self.cache.keys())
    
    def clear(self):
        for key, model in self.cache.items():
            del model
        self.cache.clear()
        torch.cuda.empty_cache()
    
    def __contains__(self, key):
        return key in self.cache

# pretrained + 10 actors = 11 max size
vq_models = LRUModelCache(max_size=MAX_CACHED_ACTORS + 1, cache_type="SoVITS")
t2s_models = LRUModelCache(max_size=MAX_CACHED_ACTORS + 1, cache_type="GPT")

# BERT Lazy Loading
tokenizer = None
bert_model = None

# cnhubert ì´ˆê¸°í™”
from feature_extractor import cnhubert
cnhubert.cnhubert_base_path = cnhubert_base_path

# í•„ìš”í•œ ëª¨ë“ˆ import
from transformers import AutoModelForMaskedLM, AutoTokenizer
import numpy as np
import librosa
import soundfile as sf
from datetime import datetime

from module.models import SynthesizerTrn
from AR.models.t2s_lightning_module import Text2SemanticLightningModule
from text import cleaned_text_to_sequence
from text.cleaner import clean_text
from time import time as ttime
from module.mel_processing import spectrogram_torch

# ssl_model (CNHubert) ì´ˆê¸°í™”
ssl_model = cnhubert.get_model()
if is_half:
    ssl_model = ssl_model.half().to(device)
else:
    ssl_model = ssl_model.to(device)

class DictToAttrRecursive(dict):
    """dictë¥¼ attributeë¡œ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ ë³€í™˜"""
    def __init__(self, input_dict):
        super().__init__(input_dict)
        for key, value in input_dict.items():
            if isinstance(value, dict):
                value = DictToAttrRecursive(value)
            self[key] = value
            setattr(self, key, value)

    def __getattr__(self, item):
        try:
            return self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")

    def __setattr__(self, key, value):
        if isinstance(value, dict):
            value = DictToAttrRecursive(value)
        super(DictToAttrRecursive, self).__setitem__(key, value)
        super().__setattr__(key, value)

    def __delattr__(self, item):
        try:
            del self[item]
        except KeyError:
            raise AttributeError(f"Attribute {item} not found")

# v2Pro ëª¨ë¸ ë¡œë” (íŠ¹ë³„í•œ í—¤ë” ì²˜ë¦¬)
from io import BytesIO

def load_sovits_new(sovits_path):
    """v2Pro/v2ProPlus ëª¨ë¸ì€ í—¤ë”ê°€ ë‹¤ë¦„ (05/06 -> PKë¡œ ë³µì› í•„ìš”)"""
    f = open(sovits_path, "rb")
    meta = f.read(2)
    if meta != b"PK":  # ì¼ë°˜ zip í˜•ì‹ì´ ì•„ë‹ˆë©´ (v2Pro/v2ProPlus)
        data = b"PK" + f.read()  # "PK" í—¤ë” ë³µì›
        bio = BytesIO()
        bio.write(data)
        bio.seek(0)
        return torch.load(bio, map_location="cpu", weights_only=False)
    return torch.load(sovits_path, map_location="cpu", weights_only=False)

# ì „ì—­ hps (ë§ˆì§€ë§‰ ë¡œë“œëœ SoVITS config)
hps = None

# def change_sovits_weights(actor, sovits_path):
#     """v2Pro SoVITS ëª¨ë¸ ë¡œë”© (LRU ìºì‹œ)"""
#     global hps, version
    
#     cached_model = vq_models.get(actor)
#     if cached_model is not None:
#         return cached_model
    
#     dict_s2 = torch.load(sovits_path, map_location="cpu", weights_only=False)
#     hps = dict_s2["config"]
#     hps = DictToAttrRecursive(hps)
#     hps.model.semantic_frame_rate = "25hz"
    
#     # v2Pro/v2ProPlus ë²„ì „ ì„¤ì •
#     hps.model.version = model_version  # "v2ProPlus"
#     version = "v2"  # symbol version for text processing
    
#     model = SynthesizerTrn(
#         hps.data.filter_length // 2 + 1,
#         hps.train.segment_size // hps.data.hop_length,
#         n_speakers=hps.data.n_speakers,
#         **hps.model
#     )
#     if "pretrained" not in sovits_path:
#         try:
#             del model.enc_q
#         except:
#             pass
#     if is_half:
#         model = model.half().to(device)
#     else:
#         model = model.to(device)
#     model.eval()
#     print(f"[v2Pro] Loading SoVITS: {sovits_path}")
#     print(model.load_state_dict(dict_s2["weight"], strict=False))
    
#     vq_models.put(actor, model)
#     return model


def change_sovits_weights(actor, sovits_path):
    """v2Pro SoVITS ëª¨ë¸ ë¡œë”© (LRU ìºì‹œ)"""
    global vq_model, hps, version, model_version
    
    # LRU ìºì‹œ ì²´í¬
    cached_model = vq_models.get(actor)
    if cached_model is not None:
        return cached_model
    
    # ========== WebUIìš© ë¡œì§ (ì‚¬ìš© ì•ˆí•¨) ==========
    # if "ï¼" in sovits_path or "!" in sovits_path:
    #     sovits_path = name2sovits_path[sovits_path]
    # version, model_version, if_lora_v3 = get_sovits_version_from_path_fast(sovits_path)
    # is_exist = is_exist_s2gv3 if model_version == "v3" else is_exist_s2gv4
    # path_sovits = path_sovits_v3 if model_version == "v3" else path_sovits_v4
    # if if_lora_v3 == True and is_exist == False:
    #     ...
    # dict_language = dict_language_v1 if version == "v1" else dict_language_v2
    # if prompt_language is not None and text_language is not None:
    #     yield (...)  # WebUI ì—…ë°ì´íŠ¸ìš©
    
    # ëª¨ë¸ ë¡œë“œ (load_sovits_new: v2Pro í—¤ë” ì²˜ë¦¬)
    dict_s2 = load_sovits_new(sovits_path)
    
    hps = dict_s2["config"]
    hps = DictToAttrRecursive(hps)
    hps.model.semantic_frame_rate = "25hz"
    
    # ë²„ì „ ê°ì§€ (v2Pro: n_speakers=300)
    if hps.data.n_speakers == 300:
        hps.model.version = "v2ProPlus"
        model_version = "v2ProPlus"
    elif "enc_p.text_embedding.weight" not in dict_s2["weight"]:
        hps.model.version = "v2"
        model_version = "v2"
    elif dict_s2["weight"]["enc_p.text_embedding.weight"].shape[0] == 322:
        hps.model.version = "v1"
        model_version = "v1"
    else:
        hps.model.version = "v2"
        model_version = "v2"
    
    version = "v2"  # text processingìš©
    print(f"[SoVITS] {sovits_path} -> version={version}, model_version={model_version}")
    
    # ========== v3/v4 ë¶„ê¸° (ì‚¬ìš© ì•ˆí•¨) ==========
    # if model_version not in v3v4set:
    #     ...
    # else:
    #     vq_model = SynthesizerTrnV3(...)
    
    # v2/v2Pro ëª¨ë¸ ìƒì„±
    vq_model = SynthesizerTrn(
        hps.data.filter_length // 2 + 1,
        hps.train.segment_size // hps.data.hop_length,
        n_speakers=hps.data.n_speakers,
        **hps.model,
    )
    
    if "pretrained" not in sovits_path:
        try:
            del vq_model.enc_q
        except:
            pass
    
    if is_half:
        vq_model = vq_model.half().to(device)
    else:
        vq_model = vq_model.to(device)
    vq_model.eval()
    
    # ========== LoRA ë¡œë”© (v2ProëŠ” LoRA ì—†ìŒ) ==========
    # if if_lora_v3 == False:
    #     ...
    # else:
    #     lora_rank = dict_s2["lora_rank"]
    #     ...
    
    print(f"loading sovits_{model_version}", vq_model.load_state_dict(dict_s2["weight"], strict=False))
    
    # ========== WebUI yield / weight.json (ì‚¬ìš© ì•ˆí•¨) ==========
    # yield (...)
    # with open("./weight.json") as f:
    #     ...
    
    # LRU ìºì‹œì— ì €ì¥
    vq_models.put(actor, vq_model)
    return vq_model

# ========== WebUI ì´ˆê¸°í™” (ì‚¬ìš© ì•ˆí•¨) ==========
# try:
#     next(change_sovits_weights(sovits_path))
# except:
#     pass



def change_gpt_weights(actor, gpt_path):
    """GPT ëª¨ë¸ ë¡œë”© (LRU ìºì‹œ)"""
    global hz, max_sec, config
    
    cached_model = t2s_models.get(actor)
    if cached_model is not None:
        return cached_model
    
    hz = 50
    dict_s1 = torch.load(gpt_path, map_location="cpu", weights_only=False)
    config = dict_s1["config"]
    max_sec = config["data"]["max_sec"]
    
    model = Text2SemanticLightningModule(config, "****", is_train=False)
    model.load_state_dict(dict_s1["weight"])
    if is_half:
        model = model.half()
    model = model.to(device)
    model.eval()
    print(f"[v2Pro] Loading GPT: {gpt_path}")
    
    t2s_models.put(actor, model)
    return model

# resample ìºì‹œ
resample_transform_dict = {}

def resample(audio_tensor, sr0, sr1, device):
    """ì˜¤ë””ì˜¤ ë¦¬ìƒ˜í”Œë§ (ìºì‹œ ì‚¬ìš©)"""
    global resample_transform_dict
    key = "%s-%s-%s" % (sr0, sr1, str(device))
    if key not in resample_transform_dict:
        resample_transform_dict[key] = torchaudio.transforms.Resample(sr0, sr1).to(device)
    return resample_transform_dict[key](audio_tensor)

def get_spepc(hps, filename, dtype, device, is_v2pro=False):
    """ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì¶”ì¶œ (v2Pro: audio_tensorë„ ë°˜í™˜)"""
    sr1 = int(hps.data.sampling_rate)
    audio, sr0 = torchaudio.load(filename)
    if sr0 != sr1:
        audio = audio.to(device)
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)
        audio = resample(audio, sr0, sr1, device)
    else:
        audio = audio.to(device)
        if audio.shape[0] == 2:
            audio = audio.mean(0).unsqueeze(0)

    maxx = audio.abs().max()
    if maxx > 1:
        audio /= min(2, maxx)
    spec = spectrogram_torch(
        audio,
        hps.data.filter_length,
        hps.data.sampling_rate,
        hps.data.hop_length,
        hps.data.win_length,
        center=False,
    )
    spec = spec.to(dtype)
    if is_v2pro:
        audio = resample(audio, sr1, 16000, device).to(dtype)
    return spec, audio

# SV (Speaker Verification) ëª¨ë¸ - v2Pro ì „ìš©
from sv import SV

sv_cn_model = None

def init_sv_cn():
    """SV ëª¨ë¸ ì´ˆê¸°í™” (v2Pro ì „ìš©)"""
    global sv_cn_model
    sv_cn_model = SV(device, is_half)
    print("[v2Pro] SV model initialized")

# v2Pro/v2ProPlusì¼ ë•Œ SV ëª¨ë¸ ì‚¬ì „ ë¡œë”©
if model_version in {"v2Pro", "v2ProPlus"}:
    init_sv_cn()

# BERT Lazy Loading í•¨ìˆ˜
def load_bert_model():
    """ì¤‘êµ­ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ BERT ëª¨ë¸ì„ Lazy Loading"""
    global tokenizer, bert_model
    if bert_model is not None:
        return
    print("ğŸ”„ Loading BERT model for Chinese language support...")
    tokenizer = AutoTokenizer.from_pretrained(bert_path)
    bert_model = AutoModelForMaskedLM.from_pretrained(bert_path)
    if is_half:
        bert_model = bert_model.half().to(device)
    else:
        bert_model = bert_model.to(device)
    print("âœ… BERT model loaded successfully!")

def get_bert_feature(text, word2ph):
    """ì¤‘êµ­ì–´ BERT feature ì¶”ì¶œ"""
    load_bert_model()
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt")
        for i in inputs:
            inputs[i] = inputs[i].to(device)
        res = bert_model(**inputs, output_hidden_states=True)
        res = torch.cat(res["hidden_states"][-3:-2], -1)[0].cpu()[1:-1]
    assert len(word2ph) == len(text)
    phone_level_feature = []
    for i in range(len(word2ph)):
        repeat_feature = res[i].repeat(word2ph[i], 1)
        phone_level_feature.append(repeat_feature)
    phone_level_feature = torch.cat(phone_level_feature, dim=0)
    return phone_level_feature.T

def clean_text_inf(text, language, version):
    phones, word2ph, norm_text = clean_text(text, language, version)
    phones = cleaned_text_to_sequence(phones, version)
    return phones, word2ph, norm_text

def get_bert_inf(phones, word2ph, norm_text, language):
    """ì–¸ì–´ë³„ BERT feature ìƒì„± - ì¤‘êµ­ì–´ë§Œ ì‹¤ì œ BERT ì‚¬ìš©"""
    language = language.replace("all_", "")
    if language == "zh":
        bert = get_bert_feature(norm_text, word2ph).to(device)
    else:
        bert = torch.zeros(
            (1024, len(phones)),
            dtype=torch.float16 if is_half else torch.float32,
        ).to(device)
    return bert

def get_first(text):
    pattern = "[" + "".join(re.escape(sep) for sep in splits) + "]"
    text = re.split(pattern, text)[0].strip()
    return text

def get_phones_and_bert(text, language, version, final=False):
    text = re.sub(r' {2,}', ' ', text)
    textlist = []
    langlist = []
    
    # ========== ì¤‘êµ­ì–´/ì›”ì–´ (ì‚¬ìš© ì•ˆí•¨) ==========
    # if language == "all_zh":
    #     for tmp in LangSegmenter.getTexts(text,"zh"):
    #         langlist.append(tmp["lang"])
    #         textlist.append(tmp["text"])
    # elif language == "all_yue":
    #     for tmp in LangSegmenter.getTexts(text,"zh"):
    #         if tmp["lang"] == "zh":
    #             tmp["lang"] = "yue"
    #         langlist.append(tmp["lang"])
    #         textlist.append(tmp["text"])
    
    # ========== ì¼ë³¸ì–´/í•œêµ­ì–´/ì˜ì–´ ==========
    if language == "all_ja":
        for tmp in LangSegmenter.getTexts(text, "ja"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "all_ko":
        for tmp in LangSegmenter.getTexts(text, "ko"):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    elif language == "en":
        langlist.append("en")
        textlist.append(text)
    elif language == "auto":
        for tmp in LangSegmenter.getTexts(text):
            langlist.append(tmp["lang"])
            textlist.append(tmp["text"])
    # elif language == "auto_yue":  # ì›”ì–´ í˜¼í•© (ì‚¬ìš© ì•ˆí•¨)
    #     for tmp in LangSegmenter.getTexts(text):
    #         if tmp["lang"] == "zh":
    #             tmp["lang"] = "yue"
    #         langlist.append(tmp["lang"])
    #         textlist.append(tmp["text"])
    else:
        # ja, ko ë“± í˜¼í•© ì²˜ë¦¬
        for tmp in LangSegmenter.getTexts(text):
            if langlist:
                if (tmp["lang"] == "en" and langlist[-1] == "en") or (tmp["lang"] != "en" and langlist[-1] != "en"):
                    textlist[-1] += tmp["text"]
                    continue
            if tmp["lang"] == "en":
                langlist.append(tmp["lang"])
            else:
                langlist.append(language)
            textlist.append(tmp["text"])
    
    print(textlist)
    print(langlist)
    phones_list = []
    bert_list = []
    norm_text_list = []
    for i in range(len(textlist)):
        lang = langlist[i]
        phones, word2ph, norm_text = clean_text_inf(textlist[i], lang, version)
        bert = get_bert_inf(phones, word2ph, norm_text, lang)
        phones_list.append(phones)
        norm_text_list.append(norm_text)
        bert_list.append(bert)
    bert = torch.cat(bert_list, dim=1)
    phones = sum(phones_list, [])
    norm_text = "".join(norm_text_list)

    if not final and len(phones) < 6:
        return get_phones_and_bert("." + text, language, version, final=True)

    return phones, bert.to(dtype), norm_text

def merge_short_text_in_array(texts, threshold):
    if len(texts) < 2:
        return texts
    result = []
    text = ""
    for ele in texts:
        text += ele
        if len(text) >= threshold:
            result.append(text)
            text = ""
    if len(text) > 0:
        if len(result) == 0:
            result.append(text)
        else:
            result[len(result) - 1] += text
    return result

def process_text(texts):
    _text = []
    if all(text in [None, " ", "\n", ""] for text in texts):
        raise ValueError('###process_text valueerror')
    for text in texts:
        if text in [None, " ", ""]:
            pass
        else:
            _text.append(text)
    return _text

def synthesize_char(char_name, audio_text, audio_language='ja', speed=1):
    import os
    
    voice_info = voice_management.get_voice_info_from_name(char_name)
    prompt_info = voice_management.get_prompt_info_from_name(char_name)
    
    if prompt_info is None:
        print(f'[synthesize_char] ERROR: No prompt info for "{char_name}"')
        return 'no info'
    if not os.path.exists(prompt_info.get('wav_path', '')):
        print(f'[synthesize_char] ERROR: Reference audio not found for "{char_name}"')
        return 'no info'
    
    use_zeroshot = False
    if voice_info is None:
        use_zeroshot = True
        print(f'[synthesize_char] No trained model for "{char_name}", using zero-shot...')
    elif not os.path.exists(voice_info.get('gpt_path', '')) or not os.path.exists(voice_info.get('sovits_path', '')):
        use_zeroshot = True
        print(f'[synthesize_char] Model files missing for "{char_name}", using zero-shot...')
    
    prompt_language = prompt_info['language']
    ref_wav_path = prompt_info['wav_path']
    prompt_text = prompt_info['text']
    
    if use_zeroshot:
        result = synthesize_cloning_voice(char_name, audio_text, audio_language, speed)
    else:
        result = get_tts_wav(ref_wav_path, prompt_text, prompt_language, audio_text, audio_language, actor=char_name, speed=speed)
    
    return result

# ========== Zero-Shot Voice Cloning ==========
# pretrained ëª¨ë¸ìš© ì „ì—­ ë³€ìˆ˜
pretrained_hps = None
PRETRAINED_ACTOR = "pretrained"  # pretrained ëª¨ë¸ìš© actor ì´ë¦„ (LRU eviction ëŒ€ìƒ ì œì™¸)

def load_pretrained_models():
    """Zero-shotìš© pretrained ëª¨ë¸ ë¡œë”© (LRU ìºì‹œì— ì €ì¥)"""
    global pretrained_hps, hps, hz, max_sec, config, model_version, version
    
    # ì´ë¯¸ ìºì‹œì— ìˆìœ¼ë©´ ìŠ¤í‚µ
    if PRETRAINED_ACTOR in vq_models.cache:
        return
    
    print("[Zero-Shot] Loading pretrained models...")
    
    # SoVITS ëª¨ë¸ ë¡œë“œ
    dict_s2 = load_sovits_new(PRETRAINED_SOVITS_PATH)
    pretrained_hps = DictToAttrRecursive(dict_s2["config"])
    pretrained_hps.model.semantic_frame_rate = "25hz"
    pretrained_hps.model.version = "v2ProPlus"
    model_version = "v2ProPlus"
    version = "v2"
    hps = pretrained_hps  # ì „ì—­ hps ì„¤ì •
    
    pretrained_vq_model = SynthesizerTrn(
        pretrained_hps.data.filter_length // 2 + 1,
        pretrained_hps.train.segment_size // pretrained_hps.data.hop_length,
        n_speakers=pretrained_hps.data.n_speakers,
        **pretrained_hps.model,
    )
    if is_half:
        pretrained_vq_model = pretrained_vq_model.half().to(device)
    else:
        pretrained_vq_model = pretrained_vq_model.to(device)
    pretrained_vq_model.eval()
    print("[Zero-Shot] SoVITS loaded:", pretrained_vq_model.load_state_dict(dict_s2["weight"], strict=False))
    
    # GPT ëª¨ë¸ ë¡œë“œ
    hz = 50
    dict_s1 = torch.load(PRETRAINED_GPT_PATH, map_location="cpu", weights_only=False)
    config = dict_s1["config"]
    max_sec = config["data"]["max_sec"]
    
    pretrained_t2s_model = Text2SemanticLightningModule(config, "****", is_train=False)
    pretrained_t2s_model.load_state_dict(dict_s1["weight"])
    if is_half:
        pretrained_t2s_model = pretrained_t2s_model.half()
    pretrained_t2s_model = pretrained_t2s_model.to(device)
    pretrained_t2s_model.eval()
    print("[Zero-Shot] GPT loaded")
    
    # LRU ìºì‹œì— ì €ì¥ (íŠ¹ë³„ actor ì´ë¦„ìœ¼ë¡œ)
    vq_models.put(PRETRAINED_ACTOR, pretrained_vq_model)
    t2s_models.put(PRETRAINED_ACTOR, pretrained_t2s_model)
    print(f"[Zero-Shot] Pretrained models cached as '{PRETRAINED_ACTOR}'")

def synthesize_cloning_voice(char_name, audio_text, audio_language='ja', speed=1):
    """
    Zero-shot ìŒì„± í´ë¡œë‹ (pretrained ëª¨ë¸ ì‚¬ìš©)
    
    Parameters:
        char_name: voices í´ë”ì˜ ìºë¦­í„° ì´ë¦„ (wav/text ì°¸ì¡°ìš©)
        audio_text: ìƒì„±í•  í…ìŠ¤íŠ¸
        audio_language: ìƒì„±í•  ì–¸ì–´ ('ja', 'ko', 'en')
        speed: ì†ë„ ë°°ìœ¨ (default: 1.0)
    
    Returns:
        str: ìƒì„±ëœ wav íŒŒì¼ ê²½ë¡œ
    
    Note:
        - actorë³„ í•™ìŠµëœ .pth ëª¨ë¸ ëŒ€ì‹  pretrained ëª¨ë¸ ì‚¬ìš©
        - voices/{char_name}/ì—ì„œ ì°¸ì¡° wavì™€ textë§Œ ì‚¬ìš©
        - ìƒˆ ìºë¦­í„° ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ (í•™ìŠµ ë¶ˆí•„ìš”)
    """
    # pretrained ëª¨ë¸ ë¡œë”© (LRU ìºì‹œì— ì €ì¥)
    load_pretrained_models()
    
    # ì°¸ì¡° ì •ë³´ ê°€ì ¸ì˜¤ê¸° (wav/textë§Œ ì‚¬ìš©, pth ë¶ˆí•„ìš”)
    prompt_info = voice_management.get_prompt_info_from_name(char_name)
    print(f"[Zero-Shot] {char_name}:", prompt_info)
    
    prompt_language = prompt_info['language']
    ref_wav_path = prompt_info['wav_path']
    prompt_text = prompt_info['text']
    
    # ê¸°ì¡´ get_tts_wav í•¨ìˆ˜ ì¬ì‚¬ìš© (actorë¥¼ PRETRAINED_ACTORë¡œ ì„¤ì •)
    result = get_tts_wav(
        ref_wav_path, prompt_text, prompt_language, 
        audio_text, audio_language, 
        actor=PRETRAINED_ACTOR, speed=speed
    )
    return result

'''
Parameters:
    ref_wav_path   : ì°¸ì¡° ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (3~10ì´ˆ ê¶Œì¥)
    prompt_text    : ì°¸ì¡° ì˜¤ë””ì˜¤ì˜ í…ìŠ¤íŠ¸ ë‚´ìš©
    prompt_language: ì°¸ì¡° ì˜¤ë””ì˜¤ ì–¸ì–´ # "ja", "ko", "en", "zh"
    text           : ìƒì„±í•  í…ìŠ¤íŠ¸
    text_language  : ìƒì„±í•  í…ìŠ¤íŠ¸ ì–¸ì–´ # "ja", "ko", "en", "zh"
    how_to_cut     : í…ìŠ¤íŠ¸ ë¶„í•  ë°©ì‹ # "No cut", "Cut every 4 sentences", "Cut every 50 characters", "Cut by Chinese period", "Cut by English period", "Cut by punctuation"
    top_k          : GPT ìƒ˜í”Œë§ top-k (default: 15)
    top_p          : GPT ìƒ˜í”Œë§ top-p (default: 1)
    temperature    : GPT ìƒ˜í”Œë§ ì˜¨ë„ (default: 1) - ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€
    ref_free       : ì°¸ì¡° ì—†ì´ ìƒì„± ì—¬ë¶€ (default: False) - v2ProëŠ” ë¯¸ì§€ì›
    speed          : ìƒì„± ì†ë„ ë°°ìœ¨ (default: 1.0)
    if_freeze      : ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (default: False)
    inp_refs       : ì¶”ê°€ ì°¸ì¡° ì˜¤ë””ì˜¤ ë¦¬ìŠ¤íŠ¸ (default: None)
    sample_steps   : v3/v4 ì „ìš© ìƒ˜í”Œ ìŠ¤í… (default: 8) - v2ProëŠ” ë¯¸ì‚¬ìš©
    if_sr          : ì˜¤ë””ì˜¤ ì´ˆê³ í•´ìƒë„ ì ìš© (default: False) - v3 ì „ìš©
    pause_second   : ë¬¸ì¥ ê°„ íœ´ì§€ ì‹œê°„ ì´ˆ (default: 0.3)
'''
# GitHub ê¸°ë³¸ê°’: top_k=20, top_p=0.6, temperature=0.6
# WebUI ë° ê¸°ì¡´ v2 ê²½í—˜ì„ í†µí•´ top_k=15, top_p=1, temperature=1ë¡œ ì¡°ì ˆ
# (zero-shotì—ì„œ EOS í† í° ëˆ„ë½ ë°©ì§€ ë° ì•ˆì •ì ì¸ ìƒì„±ì„ ìœ„í•¨)
def get_tts_wav(ref_wav_path, prompt_text, prompt_language, text, text_language, how_to_cut="No cut", top_k=15, top_p=1, temperature=1, ref_free=False, speed=1, if_freeze=False, inp_refs=None, sample_steps=8, if_sr=False, pause_second=0.3, actor='arona'):
    global cache, hps
    
    # LRU ìºì‹œì—ì„œ ëª¨ë¸ ë¡œë”© (Reference v2 ë°©ì‹)
    if actor not in vq_models:
        voice_info = voice_management.get_voice_info_from_name(actor)
        sovits_path = voice_info['sovits_path']
        print('### Actor loading', actor, ':', sovits_path)
        change_sovits_weights(actor, sovits_path)
    if actor not in t2s_models:
        voice_info = voice_management.get_voice_info_from_name(actor)
        gpt_path = voice_info['gpt_path']
        change_gpt_weights(actor, gpt_path)
    
    # ìºì‹œì—ì„œ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    vq_model = vq_models.get(actor)
    t2s_model = t2s_models.get(actor)
    
    t = []
    # if prompt_text is None or len(prompt_text) == 0:
    #     ref_free = True
    
    # if model_version in v3v4set:  # {"v3", "v4"}
    #     ref_free = False
    # else:
    #     if_sr = False
        
    ref_free = False
    if_sr = False
    
    # if model_version not in {"v3", "v4", "v2Pro", "v2ProPlus"}:
    #     clean_bigvgan_model()
    #     clean_hifigan_model()
    #     clean_sv_cn_model()
    
    t0 = ttime()
    # v2 Referenceì²˜ëŸ¼ ì§ì ‘ ì–¸ì–´ ì½”ë“œ ì‚¬ìš© (dict_language ë³€í™˜ ë¶ˆí•„ìš”)
    # prompt_language = dict_language[prompt_language]
    # text_language = dict_language[text_language]

    if not ref_free:
        prompt_text = prompt_text.strip("\n")
        if prompt_text[-1] not in splits:
            prompt_text += "ã€‚" if prompt_language != "en" else "."
        print("Actual input reference text:", prompt_text)
    
    text = text.strip("\n")
    print("Actual input target text:", text)
    
    zero_wav = np.zeros(
        int(hps.data.sampling_rate * pause_second),
        dtype=np.float16 if is_half == True else np.float32,
    )
    zero_wav_torch = torch.from_numpy(zero_wav)
    if is_half == True:
        zero_wav_torch = zero_wav_torch.half().to(device)
    else:
        zero_wav_torch = zero_wav_torch.to(device)
    
    if not ref_free:
        with torch.no_grad():
            wav16k, sr = librosa.load(ref_wav_path, sr=16000)
            if wav16k.shape[0] > 160000 or wav16k.shape[0] < 48000:
                print("Warning: Reference audio must be between 3-10 seconds")
                return None
            wav16k = torch.from_numpy(wav16k)
            if is_half == True:
                wav16k = wav16k.half().to(device)
            else:
                wav16k = wav16k.to(device)
            wav16k = torch.cat([wav16k, zero_wav_torch])
            ssl_content = ssl_model.model(wav16k.unsqueeze(0))["last_hidden_state"].transpose(1, 2)
            codes = vq_model.extract_latent(ssl_content)
            prompt_semantic = codes[0, 0]
            prompt = prompt_semantic.unsqueeze(0).to(device)

    t1 = ttime()
    t.append(t1 - t0)

    # if how_to_cut == "Cut every 4 sentences":
    #     text = cut1(text)
    # elif how_to_cut == "Cut every 50 characters":
    #     text = cut2(text)
    # elif how_to_cut == "Cut by Chinese period":
    #     text = cut3(text)
    # elif how_to_cut == "Cut by English period":
    #     text = cut4(text)
    # elif how_to_cut == "Cut by punctuation":
    #     text = cut5(text)
    
    while "\n\n" in text:
        text = text.replace("\n\n", "\n")
    
    print("Actual input target text (after sentence cutting):", text)
    texts = text.split("\n")
    texts = process_text(texts)
    texts = merge_short_text_in_array(texts, 5)
    audio_opt = []
    
    if not ref_free:
        phones1, bert1, norm_text1 = get_phones_and_bert(prompt_text, prompt_language, version)

    for i_text, text in enumerate(texts):
        if len(text.strip()) == 0:
            continue
        if text[-1] not in splits:
            text += "ã€‚" if text_language != "en" else "."
        print("Actual input target text (per sentence):", text)
        phones2, bert2, norm_text2 = get_phones_and_bert(text, text_language, version)
        print("Frontend processed text (per sentence):", norm_text2)
        
        if not ref_free:
            bert = torch.cat([bert1, bert2], 1)
            all_phoneme_ids = torch.LongTensor(phones1 + phones2).to(device).unsqueeze(0)
        else:
            bert = bert2
            all_phoneme_ids = torch.LongTensor(phones2).to(device).unsqueeze(0)

        bert = bert.to(device).unsqueeze(0)
        all_phoneme_len = torch.tensor([all_phoneme_ids.shape[-1]]).to(device)

        t2 = ttime()
        
        if i_text in cache and if_freeze == True:
            pred_semantic = cache[i_text]
        else:
            with torch.no_grad():
                pred_semantic, idx = t2s_model.model.infer_panel(
                    all_phoneme_ids,
                    all_phoneme_len,
                    None if ref_free else prompt,
                    bert,
                    top_k=top_k,
                    top_p=top_p,
                    temperature=temperature,
                    early_stop_num=hz * max_sec,
                )
                # Early stop ì•ˆì „ì¥ì¹˜: ìƒì„± ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
                if idx == 0:
                    print("[WARNING] Early stop triggered: idx==0, synthesis failed")
                    return 'early stop'
                pred_semantic = pred_semantic[:, -idx:].unsqueeze(0)
                cache[i_text] = pred_semantic
        
        t3 = ttime()
        is_v2pro = model_version in {"v2Pro", "v2ProPlus"}
        
        if True: # model_version not in v3v4set:
            refers = []
            if is_v2pro:
                sv_emb = []
                if sv_cn_model == None:
                    init_sv_cn()
            if inp_refs:
                for path in inp_refs:
                    try:
                        refer, audio_tensor = get_spepc(hps, path.name, dtype, device, is_v2pro)
                        refers.append(refer)
                        if is_v2pro:
                            sv_emb.append(sv_cn_model.compute_embedding3(audio_tensor))
                    except:
                        traceback.print_exc()
            if len(refers) == 0:
                refers, audio_tensor = get_spepc(hps, ref_wav_path, dtype, device, is_v2pro)
                refers = [refers]
                if is_v2pro:
                    sv_emb = [sv_cn_model.compute_embedding3(audio_tensor)]
            if is_v2pro:
                audio = vq_model.decode(
                    pred_semantic, torch.LongTensor(phones2).to(device).unsqueeze(0), refers, speed=speed, sv_emb=sv_emb
                )[0][0]
            else:
                audio = vq_model.decode(
                    pred_semantic, torch.LongTensor(phones2).to(device).unsqueeze(0), refers, speed=speed
                )[0][0]
        # ========== v3/v4 ë¶„ê¸° (v2Proì—ì„œëŠ” ì‚¬ìš© ì•ˆí•¨) ==========
        # else:
        #     refer, audio_tensor = get_spepc(hps, ref_wav_path, dtype, device)
        #     phoneme_ids0 = torch.LongTensor(phones1).to(device).unsqueeze(0)
        #     phoneme_ids1 = torch.LongTensor(phones2).to(device).unsqueeze(0)
        #     fea_ref, ge = vq_model.decode_encp(prompt.unsqueeze(0), phoneme_ids0, refer)
        #     ref_audio, sr = torchaudio.load(ref_wav_path)
        #     ref_audio = ref_audio.to(device).float()
        #     if ref_audio.shape[0] == 2:
        #         ref_audio = ref_audio.mean(0).unsqueeze(0)
        #     tgt_sr = 24000 if model_version == "v3" else 32000
        #     if sr != tgt_sr:
        #         ref_audio = resample(ref_audio, sr, tgt_sr, device)
        #     
        #     mel2 = mel_fn(ref_audio) if model_version == "v3" else mel_fn_v4(ref_audio)
        #     mel2 = norm_spec(mel2)
        #     T_min = min(mel2.shape[2], fea_ref.shape[2])
        #     mel2 = mel2[:, :, :T_min]
        #     fea_ref = fea_ref[:, :, :T_min]
        #     Tref = 468 if model_version == "v3" else 500
        #     Tchunk = 934 if model_version == "v3" else 1000
        #     if T_min > Tref:
        #         mel2 = mel2[:, :, -Tref:]
        #         fea_ref = fea_ref[:, :, -Tref:]
        #         T_min = Tref
        #     chunk_len = Tchunk - T_min
        #     mel2 = mel2.to(dtype)
        #     fea_todo, ge = vq_model.decode_encp(pred_semantic, phoneme_ids1, refer, ge, speed)
        #     cfm_resss = []
        #     idx = 0
        #     while 1:
        #         fea_todo_chunk = fea_todo[:, :, idx : idx + chunk_len]
        #         if fea_todo_chunk.shape[-1] == 0:
        #             break
        #         idx += chunk_len
        #         fea = torch.cat([fea_ref, fea_todo_chunk], 2).transpose(2, 1)
        #         cfm_res = vq_model.cfm.inference(
        #             fea, torch.LongTensor([fea.size(1)]).to(fea.device), mel2, sample_steps, inference_cfg_rate=0
        #         )
        #         cfm_res = cfm_res[:, :, mel2.shape[2] :]
        #         mel2 = cfm_res[:, :, -T_min:]
        #         fea_ref = fea_todo_chunk[:, :, -T_min:]
        #         cfm_resss.append(cfm_res)
        #     cfm_res = torch.cat(cfm_resss, 2)
        #     cfm_res = denorm_spec(cfm_res)
        #     if model_version == "v3":
        #         if bigvgan_model == None:
        #             init_bigvgan()
        #     else:
        #         if hifigan_model == None:
        #             init_hifigan()
        #     vocoder_model = bigvgan_model if model_version == "v3" else hifigan_model
        #     with torch.inference_mode():
        #         wav_gen = vocoder_model(cfm_res)
        #         audio = wav_gen[0][0]
        
        max_audio = torch.abs(audio).max()
        if max_audio > 1:
            audio = audio / max_audio
        audio_opt.append(audio)
        audio_opt.append(zero_wav_torch)
        t4 = ttime()
        t.extend([t2 - t1, t3 - t2, t4 - t3])
        t1 = ttime()
    
    print("%.3f\t%.3f\t%.3f\t%.3f" % (t[0], sum(t[1::3]), sum(t[2::3]), sum(t[3::3])))
    audio_opt = torch.cat(audio_opt, 0)
    
    if model_version in {"v1", "v2", "v2Pro", "v2ProPlus"}:
        opt_sr = 32000
    elif model_version == "v3":
        opt_sr = 24000
    else:
        opt_sr = 48000
    
    if if_sr == True and opt_sr == 24000:
        pass
        # v3 ì „ìš© audio super resolution (v2ProëŠ” ë¯¸ì‚¬ìš©)
    else:
        audio_opt = audio_opt.cpu().detach().numpy()
    
    # íŒŒì¼ ì €ì¥ (Reference v2 ë°©ì‹)
    global tts_idx
    tts_idx = (tts_idx + 1) % 20
    result = 'output' + str(tts_idx) + '.wav'
    
    if os.path.exists('./files_server/'):
        sf.write("./files_server/" + result, (audio_opt * 32768).astype(np.int16), opt_sr)
    # elif os.path.exists('./test/voice_inference/'):
    #     result = 'output' + actor + str(tts_idx) + '.wav'
    #     sf.write("./test/voice_inference/" + result, (audio_opt * 32768).astype(np.int16), opt_sr)
    else:
        sf.write("./" + result, (audio_opt * 32768).astype(np.int16), opt_sr)
    
    # Testìš© íŒŒì¼ì €ì¥
    try:
        voice_file_name = actor + "_voice_" + str(datetime.now().strftime("%y%m%d_%H%M%S")) + "_" + text + ".wav"
        voice_audio_path = os.path.join('./test/voice', voice_file_name)
        os.makedirs('./test/voice', exist_ok=True)
        sf.write(voice_audio_path, (audio_opt * 32768).astype(np.int16), opt_sr)
    except:
        print('fail saving get_tts_wav')
    
    return result


if __name__ == '__main__':    
    # TODO : ë¡œì»¬í™”í•  ê²½ìš°, ì˜í–¥ë„ íŒŒì•… (í˜„ì¬ íŒ¨í‚¤ì§•ì€ ê°€ëŠ¥)
    import nltk
    nltk.download('averaged_perceptron_tagger_eng')
    
    cnhubert_base_path = './pretrained_models/chinese-hubert-base'
    bert_path = './pretrained_models/chinese-roberta-wwm-ext-large'
    _CUDA_VISIBLE_DEVICES = 0
    # is_half = False  # Float32
    is_half = True  # Float32
    
    # audio_text = 'ì•ˆë…•? ë‚œ ë¯¼êµ¬ë¼ê³  í•´'
    # audio_text = 'í…ŒìŠ¤íŠ¸ì¤‘! ë©”ë¦¬í¬ë¦¬ìŠ¤ë§ˆìŠ¤!'
    # audio_text = 'API ì‚¬ìš© ê°€ëŠ¥í•œê°€ìš”?'
    # audio_text = 'Only English Spokened'
    # audio_text = 'ì˜¤ì¼€ì´!'
    # audio_text = 'python can be spoken'
    # audio_text = 'get some rest sensei! ì•ˆë…•í•˜ì„¸ìš”?'
    # audio_language = 'ko'
    audio_text = 'å¾…ã£ã¦ãŠã£ãŸãã€å…ˆç”Ÿã€‚'
    audio_text = 'ããªãŸã¯ã‚¤ã‚¿ã‚ºãƒ©ãŒå¥½ããªã®ã˜ã‚ƒãªã€‚'
    # audio_text = 'æ–°ã—ãã‚’çŸ¥ã‚‹ã®ã¯è‰¯ã„ã“ã¨ã˜ã‚ƒã€‚ãã†ã˜ã‚ƒã‚?'
    # audio_text = 'ã˜ã‚ƒãŒã€ã‚†ã‚‹ãã†ã€‚'
    # audio_text = 'ã»ã‚Œã€ã‚«ãƒœãƒãƒ£ã¨ãƒŠãƒ„ãƒ¡ã®æ–™ç†ã˜ã‚ƒã€‚ããªãŸã¨ä¸€ç·’ã«é£Ÿã¹ãŸã„ã¨æ€ã£ã¦ãªã€‚'
    audio_text = 'å³ã‚¯ãƒªãƒƒã‚¯ã§ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’é–‹ãã€è¨­å®šã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚'
    # audio_text = 'ãµã…ãˆâ€¦'
    # audio_text = 'ã²ãˆãˆãˆã£ï¼'
    # audio_text = 'å…ˆç”Ÿã‚’ä¿¡ç”¨ã—ã¦ã„ã‚‹ã¤ã‚‚ã‚Šã§ã™ã€‚'  # miyako idx = 0 ì˜¤ë¥˜
    audio_language = 'ja'
    # print('error?')
    
    audio_text = audio_text.replace('AI', 'ãˆã„ã‚ã„')
    audio_text = audio_text.replace('MY-Little-JARVIS-3D', 'ãƒã‚¤ãƒªãƒˆãƒ«ãƒ»ã‚¸ãƒ£ãƒ¼ãƒ“ã‚¹ ã‚¹ãƒªãƒ¼ã§ãƒ')
    audio_text = audio_text.replace('MY-Little-JARVIS', 'ãƒã‚¤ãƒªãƒˆãƒ«ãƒ»ã‚¸ãƒ£ãƒ¼ãƒ“ã‚¹')
    audio_text = audio_text.replace('Android', 'ã‚¢ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ‰')
    audio_text = audio_text.replace('Windows', 'ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚º')
    audio_text = audio_text.replace('æ–¹', 'ã‹ãŸ')
    audio_text = audio_text.replace('.exe', 'ãƒ‰ãƒƒãƒˆ exe')
    
    print(audio_text)
    
    actor = 'arona'

    prompt_info = voice_management.get_prompt_info_from_name(actor)  # Todo : ì—†ì„ë•Œì˜ Try Catch
    prompt_language = prompt_info['language'] # 'ja'
    ref_wav_path = prompt_info['wav_path'] #'./voices/noa.wav'
    prompt_text = prompt_info['text'] # 'ã•ã™ãŒã§ã™ã€å…ˆç”Ÿã€‚å‹‰å¼·ã«ãªã‚Šã¾ã—ãŸã€‚'
       
    # get_tts_wav(ref_wav_path, prompt_text, prompt_language, audio_text, audio_language, actor=actor)
    # get_tts_wav(ref_wav_path, prompt_text, prompt_language, audio_text, audio_language, actor='arona')
    # get_tts_wav(ref_wav_path, prompt_text, prompt_language, audio_text, audio_language, actor='arona')
    # get_tts_wav(ref_wav_path, prompt_text, prompt_language, audio_text, audio_language, actor='arona')

    result = synthesize_char(actor, audio_text, audio_language='ja', speed=1)
    print('save at ' + result)
    
    # ========== Arona 30ê°œ ë¬¸ì¥ ì—°ì† í…ŒìŠ¤íŠ¸ ==========
    if False:
        print("\n" + "="*60)
        print("[Arona Multi-Sentence Test] Testing 30 sentences...")
        print("="*60 + "\n")
        
        audio_text_list = []
        # ì¼ë³¸ì–´ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ 30ê°œ
        audio_text_list.append("ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€å…ˆç”Ÿã€‚ä»Šæ—¥ã‚‚è‰¯ã„ä¸€æ—¥ã«ãªã‚Šã¾ã™ã‚ˆã†ã«ã€‚")
        audio_text_list.append("å…ˆç”Ÿã€ãŠç–²ã‚Œæ§˜ã§ã™ã€‚å°‘ã—ä¼‘æ†©ã—ã¾ã›ã‚“ã‹ï¼Ÿ")
        audio_text_list.append("ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã¿ã¾ã—ãŸãŒã€èˆˆå‘³æ·±ã„çµæœãŒå‡ºã¾ã—ãŸã€‚")
        audio_text_list.append("ã‚·ãƒ£ãƒ¼ãƒ¬ã®ç”Ÿå¾’ãŸã¡ã¯ã€ã¿ã‚“ãªå…ƒæ°—ã«ã—ã¦ã„ã¾ã™ã€‚")
        audio_text_list.append("ä»Šæ—¥ã®æˆæ¥­ã€ã¨ã¦ã‚‚æ¥½ã—ã‹ã£ãŸã§ã™ã­ã€‚")
        audio_text_list.append("å…ˆç”Ÿã€æ˜æ—¥ã®äºˆå®šã¯ç¢ºèªã•ã‚Œã¾ã—ãŸã‹ï¼Ÿ")
        audio_text_list.append("æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒå§‹ã¾ã‚Šã¾ã™ã‚ˆã€‚")
        audio_text_list.append("ã¿ã‚“ãªã§å”åŠ›ã™ã‚Œã°ã€å¿…ãšæˆåŠŸã§ãã¾ã™ã€‚")
        audio_text_list.append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        audio_text_list.append("ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚·ã‚¹ãƒ†ãƒ ã«ç•°å¸¸ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        audio_text_list.append("å…ˆç”Ÿã®ãŠã‹ã’ã§ã€å•é¡ŒãŒè§£æ±ºã—ã¾ã—ãŸã€‚")
        audio_text_list.append("ã“ã®è³‡æ–™ã€ã¨ã¦ã‚‚å‚è€ƒã«ãªã‚‹ã¨æ€ã„ã¾ã™ã€‚")
        audio_text_list.append("æ¬¡ã®ä¼šè­°ã¯åˆå¾Œ2æ™‚ã‹ã‚‰ã§ã™ã€‚")
        audio_text_list.append("ãŠæ˜¼ã”é£¯ã€ä½•ã‚’é£Ÿã¹ã¾ã™ã‹ï¼Ÿ")
        audio_text_list.append("å¤©æ°—ãŒè‰¯ã„ã§ã™ã­ã€‚æ•£æ­©ã—ã¾ã›ã‚“ã‹ï¼Ÿ")
        audio_text_list.append("æ–°ã—ã„æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ã¿ã¾ã—ãŸã€‚")
        audio_text_list.append("ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã¯çµ‚ã‚ã‚Šã¾ã—ãŸã€‚")
        audio_text_list.append("ãƒ¬ãƒãƒ¼ãƒˆã®æå‡ºæœŸé™ã¯æ˜æ—¥ã¾ã§ã§ã™ã€‚")
        audio_text_list.append("ã¿ã‚“ãªã€é ‘å¼µã£ã¦ã„ã¾ã™ã­ã€‚")
        audio_text_list.append("å…ˆç”Ÿã€è³ªå•ãŒã‚ã‚Šã¾ã™ã€‚èã„ã¦ã‚‚ã„ã„ã§ã™ã‹ï¼Ÿ")
        audio_text_list.append("è¨ˆç”»é€šã‚Šã«é€²ã‚“ã§ã„ã¾ã™ã€‚")
        audio_text_list.append("ã‚‚ã†ã™ãã‚´ãƒ¼ãƒ«ãŒè¦‹ãˆã¦ãã¾ã—ãŸã€‚")
        audio_text_list.append("ç´ æ™´ã‚‰ã—ã„æˆæœã§ã™ã­ã€‚ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ã€‚")
        audio_text_list.append("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã¿ã¾ã—ã‚‡ã†ã€‚")
        audio_text_list.append("å›°ã£ãŸã“ã¨ãŒã‚ã‚Œã°ã€ã„ã¤ã§ã‚‚ç›¸è«‡ã—ã¦ãã ã•ã„ã€‚")
        audio_text_list.append("å…ˆç”Ÿã¯æœ¬å½“ã«é ¼ã‚Šã«ãªã‚Šã¾ã™ã€‚")
        audio_text_list.append("ä»Šæ—¥ã‚‚ä¸€æ—¥ã€ãŠç–²ã‚Œæ§˜ã§ã—ãŸã€‚")
        audio_text_list.append("æ˜æ—¥ã‚‚é ‘å¼µã‚Šã¾ã—ã‚‡ã†ã­ã€‚")
        audio_text_list.append("è‰¯ã„å¤¢ã‚’è¦‹ã¦ãã ã•ã„ã€‚ãŠã‚„ã™ã¿ãªã•ã„ã€‚")
        audio_text_list.append("ã¾ãŸæ˜æ—¥ãŠä¼šã„ã—ã¾ã—ã‚‡ã†ã€‚ã•ã‚ˆã†ãªã‚‰ã€‚")
        
        for idx, text in enumerate(audio_text_list, 1):
            try:
                print(f"[{idx}/30] Synthesizing: {text[:30]}...")
                result = synthesize_char('arona', text, audio_language='ja', speed=1)
                result_path = os.path.abspath(result)
                print(f"[{idx}/30] -> {result_path} âœ“")
            except Exception as e:
                print(f"[{idx}/30] -> FAILED: {e}")
        
        print("\n" + "="*60)
        print("[Arona Multi-Sentence Test] Completed!")
        print("="*60)
    
    # ========== Zero-Shot Voice Cloning ì „ì²´ ìºë¦­í„° í…ŒìŠ¤íŠ¸ ==========
    if True:
        print("\n" + "="*60)
        print("[Zero-Shot] Testing all characters...")
        print("="*60 + "\n")
        
        # info.jsonì˜ ëª¨ë“  ìºë¦­í„° ëª©ë¡
        all_characters = [
            'arona', 'plana', 'mika', 'yuuka', 'noa', 'koyuki',
            'nagisa', 'mari', 'kisaki', 'miyako', 'ui', 'seia', 'prana'
        ]
        
        test_text = "å…ˆç”Ÿã€ãŠç–²ã‚Œæ§˜ã§ã—ãŸã€‚ä»Šæ—¥ã‚‚é ‘å¼µã‚Šã¾ã—ãŸã­ã€‚"  # í…ŒìŠ¤íŠ¸ìš© ì¼ë³¸ì–´ í…ìŠ¤íŠ¸
        
        for char in all_characters:
            try:
                print(f"\n[Zero-Shot Test] {char}...")
                result = synthesize_cloning_voice(char, test_text, audio_language='ja', speed=1)
                # Ctrl+í´ë¦­ ê°€ëŠ¥í•œ ì ˆëŒ€ ê²½ë¡œë¡œ ì¶œë ¥
                result_path = os.path.abspath(result)
                print(f"[Zero-Shot Test] {char} -> {result_path} âœ“")
            except Exception as e:
                print(f"[Zero-Shot Test] {char} -> FAILED: {e}")
        
        print("\n" + "="*60)
        print("[Zero-Shot] All tests completed!")
        print("="*60)